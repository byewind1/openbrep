"""
gdl-agent Web UI â€” Streamlit interface for architects.

Run: streamlit run ui/app.py
"""

import sys
import re
import os
import time
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import streamlit as st

from gdl_agent.hsf_project import HSFProject, ScriptType, GDLParameter
from gdl_agent.gdl_parser import parse_gdl_source, parse_gdl_file
from gdl_agent.paramlist_builder import build_paramlist_xml, validate_paramlist
from gdl_agent.compiler import MockHSFCompiler, HSFCompiler, CompileResult
from gdl_agent.core import GDLAgent, Status
from gdl_agent.knowledge import KnowledgeBase
from gdl_agent.skills_loader import SkillsLoader


# â”€â”€ Page Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.set_page_config(
    page_title="gdl-agent",
    page_icon="ğŸ—ï¸",
    layout="wide",
    initial_sidebar_state="expanded",
)

# â”€â”€ Custom CSS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;600&family=Noto+Sans+SC:wght@300;400;600&display=swap');

.stApp { font-family: 'Noto Sans SC', sans-serif; }
code, .stCodeBlock { font-family: 'JetBrains Mono', monospace !important; }

.main-header {
    font-family: 'JetBrains Mono', monospace;
    font-size: 2rem; font-weight: 600;
    background: linear-gradient(135deg, #22d3ee, #34d399);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    margin-bottom: 0;
}
.sub-header { color: #94a3b8; font-size: 0.9rem; margin-top: -0.5rem; margin-bottom: 2rem; }

.welcome-card {
    background: linear-gradient(135deg, #0f172a, #1e293b);
    border: 1px solid #334155;
    border-radius: 12px;
    padding: 2rem;
    margin: 1rem 0;
}
.step-item {
    display: flex;
    align-items: flex-start;
    gap: 0.75rem;
    margin-bottom: 1rem;
    padding: 0.75rem;
    background: #1e293b;
    border-radius: 8px;
    border-left: 3px solid #22d3ee;
}
</style>
""", unsafe_allow_html=True)


# â”€â”€ Session State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if "project" not in st.session_state:
    st.session_state.project = None
if "compile_log" not in st.session_state:
    st.session_state.compile_log = []
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "work_dir" not in st.session_state:
    st.session_state.work_dir = str(Path.home() / "gdl-agent-workspace")
if "agent_running" not in st.session_state:
    st.session_state.agent_running = False
if "pending_changes" not in st.session_state:
    # Stores AI-generated code waiting for user confirmation before compile
    # Structure: {"gsm_name": str, "changes": {path: content}, "instruction": str}
    st.session_state.pending_changes = None
if "model_api_keys" not in st.session_state:
    # Per-model API Key storage â€” pre-fill from config.toml provider_keys
    st.session_state.model_api_keys = {}


# â”€â”€ Load config.toml defaults â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_config_defaults = {}
_provider_keys: dict = {}   # {provider: api_key}

try:
    from gdl_agent.config import GDLAgentConfig
    import sys as _sys, os as _os
    # Load raw TOML to get provider_keys nested table
    if _sys.version_info >= (3, 11):
        import tomllib as _tomllib
    else:
        import tomli as _tomllib   # type: ignore

    _toml_path = _os.path.join(_os.path.dirname(__file__), "..", "config.toml")
    if _os.path.exists(_toml_path):
        with open(_toml_path, "rb") as _f:
            _raw = _tomllib.load(_f)
        _provider_keys = _raw.get("llm", {}).get("provider_keys", {})

    _config = GDLAgentConfig.load()
    _config_defaults = {
        "llm_model": _config.llm.model,
        "compiler_path": _config.compiler.path or "",
    }
except Exception:
    pass


def _key_for_model(model: str) -> str:
    """Pick the right API Key from provider_keys based on model name."""
    m = model.lower()
    if "glm" in m:
        return _provider_keys.get("zhipu", "")
    elif "deepseek" in m and "ollama" not in m:
        return _provider_keys.get("deepseek", "")
    elif "claude" in m:
        return _provider_keys.get("anthropic", "")
    elif "gpt" in m or "o3" in m or "o1" in m:
        return _provider_keys.get("openai", "")
    elif "gemini" in m:
        return _provider_keys.get("google", "")
    return ""

# â”€â”€ Sidebar Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with st.sidebar:
    st.markdown('<p class="main-header">gdl-agent</p>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">v0.4.1 Â· HSF-native Â· AI-powered</p>', unsafe_allow_html=True)
    st.divider()

    st.subheader("ğŸ“ å·¥ä½œç›®å½•")
    work_dir = st.text_input("Work Directory", value=st.session_state.work_dir, label_visibility="collapsed")
    st.session_state.work_dir = work_dir

    st.divider()
    st.subheader("ğŸ”§ ç¼–è¯‘å™¨ / Compiler")

    compiler_mode = st.radio(
        "ç¼–è¯‘æ¨¡å¼",
        ["Mock (æ— éœ€ ArchiCAD)", "LP_XMLConverter (çœŸå®ç¼–è¯‘)"],
        index=1 if _config_defaults.get("compiler_path") else 0,
    )

    converter_path = ""
    if compiler_mode.startswith("LP"):
        converter_path = st.text_input(
            "LP_XMLConverter è·¯å¾„",
            value=_config_defaults.get("compiler_path", ""),
            placeholder="/Applications/GRAPHISOFT/ArchiCAD 28/LP_XMLConverter",
        )

    st.divider()
    st.subheader("ğŸ§  AI æ¨¡å‹ / LLM")

    model_options = [
        # â”€â”€ Anthropic Claude â”€â”€
        "claude-haiku-4-5-20251001",
        "claude-sonnet-4-5-20250929",
        "claude-opus-4-5-20250918",
        "claude-opus-4-6",
        # â”€â”€ æ™ºè°± GLM (Z.ai) â”€â”€
        "glm-4.7",
        "glm-4.7-flash",
        "glm-4-plus",
        "glm-4-flash",
        # â”€â”€ OpenAI â”€â”€
        "gpt-4o",
        "gpt-4o-mini",
        "o3-mini",
        # â”€â”€ DeepSeek â”€â”€
        "deepseek-chat",
        "deepseek-reasoner",
        # â”€â”€ Google Gemini â”€â”€
        "gemini/gemini-2.5-flash",
        "gemini/gemini-2.5-pro",
        # â”€â”€ Ollama æœ¬åœ° â”€â”€
        "ollama/qwen2.5:14b",
        "ollama/qwen3:8b",
        "ollama/deepseek-coder-v2:16b",
    ]

    default_model = _config_defaults.get("llm_model", "glm-4.7")
    default_index = model_options.index(default_model) if default_model in model_options else 4

    model_name = st.selectbox("æ¨¡å‹ / Model", model_options, index=default_index)

    # Load or initialize API Key for this specific model
    if model_name not in st.session_state.model_api_keys:
        # Auto-fill from config.toml provider_keys
        st.session_state.model_api_keys[model_name] = _key_for_model(model_name)

    api_key = st.text_input(
        "API Key",
        value=st.session_state.model_api_keys.get(model_name, ""),
        type="password",
        help="Ollama æœ¬åœ°æ¨¡å¼ä¸éœ€è¦ Key"
    )

    # Auto-save API Key if user manually edited it
    if api_key != st.session_state.model_api_keys.get(model_name, ""):
        st.session_state.model_api_keys[model_name] = api_key

    if "claude" in model_name:
        st.caption("ğŸ”‘ [è·å– Claude API Key â†’](https://console.anthropic.com/settings/keys)")
        st.caption("âš ï¸ API Key éœ€å•ç‹¬å……å€¼ï¼Œä¸ Claude Pro è®¢é˜…é¢åº¦æ— å…³")
    elif "glm" in model_name:
        st.caption("ğŸ”‘ [è·å–æ™ºè°± API Key â†’](https://bigmodel.cn/usercenter/apikeys)")
    elif "gpt" in model_name or "o3" in model_name:
        st.caption("ğŸ”‘ [è·å– OpenAI API Key â†’](https://platform.openai.com/api-keys)")
    elif "deepseek" in model_name and "ollama" not in model_name:
        st.caption("ğŸ”‘ [è·å– DeepSeek API Key â†’](https://platform.deepseek.com/api_keys)")
    elif "gemini" in model_name:
        st.caption("ğŸ”‘ [è·å– Gemini API Key â†’](https://aistudio.google.com/apikey)")
    elif "ollama" in model_name:
        st.caption("ğŸ–¥ï¸ æœ¬åœ°è¿è¡Œï¼Œæ— éœ€ Keyã€‚ç¡®ä¿ Ollama å·²å¯åŠ¨ã€‚")

    # API Base URL â€” only needed for OpenAI-compatible custom endpoints
    # zai/ (GLM), deepseek/, anthropic/ are native LiteLLM providers, no api_base needed
    def _get_default_api_base(model: str) -> str:
        m = model.lower()
        if "ollama" in m:
            return "http://localhost:11434"
        # GLM uses zai/ native provider â€” no api_base
        # DeepSeek uses deepseek/ native provider â€” no api_base
        return ""

    default_api_base = _get_default_api_base(model_name)
    api_base = ""
    if default_api_base:
        api_base = st.text_input("API Base URL", value=default_api_base)

    max_retries = st.slider("æœ€å¤§é‡è¯•æ¬¡æ•°", 1, 10, 5)

    st.divider()

    # Project info + quick reset
    if st.session_state.project:
        proj = st.session_state.project
        st.subheader(f"ğŸ“¦ {proj.name}")
        st.caption(f"å‚æ•°: {len(proj.parameters)} | è„šæœ¬: {len(proj.scripts)}")
        if st.button("ğŸ—‘ï¸ æ¸…é™¤é¡¹ç›®", use_container_width=True):
            st.session_state.project = None
            st.session_state.chat_history = []
            st.rerun()


# â”€â”€ Helper Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_compiler():
    if compiler_mode.startswith("Mock"):
        return MockHSFCompiler()
    return HSFCompiler(converter_path or None)

def get_llm():
    from gdl_agent.config import LLMConfig
    from gdl_agent.llm import LLMAdapter
    config = LLMConfig(
        model=model_name,
        api_key=api_key,
        api_base=api_base,
        temperature=0.2,
        max_tokens=4096,
    )
    return LLMAdapter(config)

def load_knowledge(task_type: str = "all"):
    kb_dir = Path(st.session_state.work_dir) / "knowledge"
    if not kb_dir.exists():
        kb_dir = Path(__file__).parent.parent / "knowledge"
    kb = KnowledgeBase(str(kb_dir))
    kb.load()
    return kb.get_by_task_type(task_type)

def load_skills():
    sk_dir = Path(st.session_state.work_dir) / "skills"
    if not sk_dir.exists():
        sk_dir = Path(__file__).parent.parent / "skills"
    sl = SkillsLoader(str(sk_dir))
    sl.load()
    return sl

def _versioned_gsm_path(proj_name: str, work_dir: str) -> str:
    """
    Return next available versioned GSM path.
    MyShelf_v1.gsm â†’ MyShelf_v2.gsm â†’ ...
    Preserves all previous compilations.
    """
    out_dir = Path(work_dir) / "output"
    out_dir.mkdir(parents=True, exist_ok=True)
    v = 1
    while (out_dir / f"{proj_name}_v{v}.gsm").exists():
        v += 1
    return str(out_dir / f"{proj_name}_v{v}.gsm")


# â”€â”€ Object Name Extraction (dictionary + regex, no LLM) â”€â”€

_CN_TO_NAME = {
    # å®¶å…·
    "ä¹¦æ¶": "Bookshelf", "ä¹¦æŸœ": "Bookcase", "æŸœå­": "Cabinet", "è¡£æŸœ": "Wardrobe",
    "æ¡Œå­": "Table", "æ¡Œ": "Table", "ä¹¦æ¡Œ": "Desk", "é¤æ¡Œ": "DiningTable",
    "æ¤…å­": "Chair", "æ¤…": "Chair", "æ²™å‘": "Sofa", "åºŠ": "Bed",
    "èŒ¶å‡ ": "CoffeeTable", "ç”µè§†æŸœ": "TVStand", "é‹æŸœ": "ShoeRack",
    # å»ºç­‘æ„ä»¶
    "çª—": "Window", "çª—æ¡†": "WindowFrame", "çª—æˆ·": "Window", "ç™¾å¶çª—": "Louver",
    "é—¨": "Door", "é—¨æ¡†": "DoorFrame", "æ¨æ‹‰é—¨": "SlidingDoor", "æ—‹è½¬é—¨": "RevolvingDoor",
    "å¢™": "Wall", "å¢™æ¿": "WallPanel", "éš”å¢™": "Partition", "å¹•å¢™": "CurtainWall",
    "æ¥¼æ¢¯": "Staircase", "å°é˜¶": "StairStep", "æ‰¶æ‰‹": "Handrail", "æ æ†": "Railing",
    "æŸ±": "Column", "æŸ±å­": "Column", "æ¢": "Beam", "æ¿": "Slab",
    "å±‹é¡¶": "Roof", "å¤©èŠ±": "Ceiling", "åœ°æ¿": "Floor",
    # è®¾å¤‡
    "ç¯": "Light", "ç¯å…·": "LightFixture", "ç®¡é“": "Pipe", "é£ç®¡": "Duct",
    "å¼€å…³": "Switch", "æ’åº§": "Outlet", "ç©ºè°ƒ": "AirConditioner",
    # æ™¯è§‚
    "èŠ±ç›†": "Planter", "æ ‘": "Tree", "å›´æ ": "Fence", "é•¿å‡³": "Bench",
}

def _extract_object_name(text: str) -> str:
    """
    Extract GDL object name from user input.
    Priority: explicit English name > Chinese keyword dict > fallback.
    Zero LLM calls â€” instant and 100% reliable.
    """
    # 1. Check for explicit English name: "named MyShelf", "å« MyShelf"
    for pat in [
        r'named?\s+([A-Za-z][A-Za-z0-9]{2,30})',
        r'called\s+([A-Za-z][A-Za-z0-9]{2,30})',
        r'åä¸º\s*([A-Za-z][A-Za-z0-9]{2,30})',
        r'å«\s*([A-Za-z][A-Za-z0-9]{2,30})',
    ]:
        m = re.search(pat, text, re.IGNORECASE)
        if m:
            return m.group(1)

    # 2. Chinese keyword â†’ English CamelCase (longest match first)
    for cn, en in sorted(_CN_TO_NAME.items(), key=lambda x: len(x[0]), reverse=True):
        if cn in text:
            print(f"[name] '{cn}' â†’ {en}")
            return en

    # 3. Pick first CamelCase English word in text (skip short junk like UI, AI, GDL)
    for word in re.findall(r'[A-Z][a-z]{2,}[A-Za-z0-9]*', text):
        if word not in {"The", "For", "And", "Not", "But", "With"}:
            return word

    return "MyObject"


# â”€â”€ Welcome / Onboarding Panel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def show_welcome():
    st.markdown("""
<div class="welcome-card">
<h2 style="color:#22d3ee; margin-top:0; font-family:'JetBrains Mono';">æ¬¢è¿ä½¿ç”¨ gdl-agent ğŸ—ï¸</h2>
<p style="color:#94a3b8;">ç”¨è‡ªç„¶è¯­è¨€é©±åŠ¨ ArchiCAD GDL å¯¹è±¡çš„åˆ›å»ºä¸ç¼–è¯‘ã€‚æ— éœ€äº†è§£ GDL è¯­æ³•ï¼Œç›´æ¥æè¿°éœ€æ±‚å³å¯ã€‚</p>
</div>
""", unsafe_allow_html=True)

    st.markdown("#### ä¸‰æ­¥å¿«é€Ÿå¼€å§‹")

    st.info("**â‘  é…ç½® API Key**  \nåœ¨å·¦ä¾§è¾¹æ é€‰æ‹© AI æ¨¡å‹ï¼Œå¡«å…¥å¯¹åº” API Keyã€‚å…è´¹çš„æ™ºè°± GLM å¯ç›´æ¥ä½¿ç”¨ã€‚")
    st.info("**â‘¡ å¼€å§‹å¯¹è¯**  \nåœ¨åº•éƒ¨è¾“å…¥æ¡†æè¿°ä½ æƒ³åˆ›å»ºçš„ GDL å¯¹è±¡ï¼Œä¾‹å¦‚ï¼š  \nã€Œåˆ›å»ºä¸€ä¸ªå®½ 600mmã€æ·± 400mm çš„ä¹¦æ¶ï¼Œå¸¦ iShelves å‚æ•°æ§åˆ¶å±‚æ•°ã€")
    st.info("**â‘¢ ç¼–è¯‘è¾“å‡º**  \nAI ç”Ÿæˆä»£ç åè‡ªåŠ¨è§¦å‘ç¼–è¯‘ã€‚çœŸå®ç¼–è¯‘éœ€åœ¨ä¾§è¾¹æ é…ç½® LP_XMLConverter è·¯å¾„ã€‚Mock æ¨¡å¼å¯éªŒè¯ç»“æ„ï¼Œæ— éœ€å®‰è£… ArchiCADã€‚")

    st.divider()

    st.markdown("#### æˆ–è€…ï¼šå¯¼å…¥å·²æœ‰ GDL æ–‡ä»¶")
    uploaded_file = st.file_uploader(
        "æ‹–å…¥ .gdl æ–‡ä»¶å¼€å§‹ç¼–è¾‘",
        type=["gdl", "txt"],
        help="æ”¯æŒ AI ç”Ÿæˆæˆ–æ‰‹å†™çš„ GDL æºç ",
        key="welcome_upload",
    )
    if uploaded_file:
        content = uploaded_file.read().decode("utf-8", errors="replace")
        name = Path(uploaded_file.name).stem
        try:
            project = parse_gdl_source(content, name)
            project.work_dir = Path(st.session_state.work_dir)
            project.root = project.work_dir / project.name
            st.session_state.project = project
            st.session_state.chat_history.append({
                "role": "assistant",
                "content": f"âœ… å·²å¯¼å…¥ `{project.name}` â€” {len(project.parameters)} ä¸ªå‚æ•°ï¼Œ{len(project.scripts)} ä¸ªè„šæœ¬ã€‚å¯ä»¥å¼€å§‹å¯¹è¯ä¿®æ”¹äº†ã€‚"
            })
            st.rerun()
        except Exception as e:
            st.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")

    st.divider()
    st.caption("ğŸ’¡ æç¤ºï¼šç¬¬ä¸€æ¡æ¶ˆæ¯æ— éœ€åˆ›å»ºé¡¹ç›®ï¼Œç›´æ¥æè¿°éœ€æ±‚ï¼ŒAI ä¼šè‡ªåŠ¨åˆå§‹åŒ–ã€‚")


# â”€â”€ Intent Classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_GDL_KEYWORDS = [
    "åˆ›å»º", "ç”Ÿæˆ", "åˆ¶ä½œ", "åšä¸€ä¸ª", "å»ºä¸€ä¸ª", "å†™ä¸€ä¸ª",
    "ä¿®æ”¹", "æ›´æ–°", "æ·»åŠ ", "åˆ é™¤", "è°ƒæ•´", "ä¼˜åŒ–",
    "ä¹¦æ¶", "æŸœå­", "çª—", "é—¨", "å¢™", "æ¥¼æ¢¯", "æ¡Œ", "æ¤…",
    "å‚æ•°", "parameter", "script", "gdl", "gsm", "hsf",
    "compile", "ç¼–è¯‘", "build", "create", "make", "add",
    "3d", "2d", "prism", "block", "sphere",
]

def _is_gdl_intent(text: str) -> bool:
    """Quick keyword check â€” if obvious GDL request, skip LLM classification."""
    t = text.lower()
    return any(kw in t for kw in _GDL_KEYWORDS)

def classify_and_extract(text: str, llm) -> tuple:
    """
    Returns: (intent, obj_name)
    - intent: via keyword fast-path, LLM only for ambiguous cases
    - obj_name: dictionary + regex, zero LLM calls
    """
    # Name: always from dictionary (instant, no LLM)
    obj_name = _extract_object_name(text)

    # Intent fast-path
    if _is_gdl_intent(text):
        return ("GDL", obj_name)

    # Ambiguous â†’ ask LLM just for GDL/CHAT (one word)
    try:
        resp = llm.generate([
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯æ„å›¾åˆ†ç±»å™¨ã€‚åˆ¤æ–­ç”¨æˆ·æ˜¯å¦æƒ³åˆ›å»ºæˆ–ä¿®æ”¹ ArchiCAD GDL æ„ä»¶ã€‚\n"
                    "åªå›å¤ä¸€ä¸ªè¯ï¼šGDL æˆ– CHAT\n"
                    "GDL = è¦åˆ›å»º/ä¿®æ”¹/ç¼–è¯‘æ„ä»¶\n"
                    "CHAT = é—²èŠ/æ‰“æ‹›å‘¼/é—®ç”¨æ³•"
                ),
            },
            {"role": "user", "content": text},
        ], max_tokens=10, temperature=0.1)

        raw = resp.content.strip().upper()
        print(f"[classify] intent raw: '{raw}'")
        intent = "GDL" if "GDL" in raw else "CHAT"
        return (intent, obj_name)

    except Exception as e:
        print(f"[classify] exception: {e}")
        return ("CHAT", obj_name)


def chat_respond(user_input: str, history: list, llm) -> str:
    """Simple conversational response without triggering Agent."""
    system_msg = {
        "role": "system",
        "content": (
            "ä½ æ˜¯ gdl-agent çš„åŠ©æ‰‹ï¼Œä¸“æ³¨äº ArchiCAD GDL åº“æ„ä»¶çš„åˆ›å»ºä¸ç¼–è¯‘ã€‚"
            "ç”¨æˆ·å¯ä»¥å’Œä½ é—²èŠï¼Œä¹Ÿå¯ä»¥è®©ä½ åˆ›å»º GDL å¯¹è±¡ã€‚"
            "é—²èŠæ—¶è‡ªç„¶å›åº”ï¼Œç®€æ´å‹å¥½ï¼›æ¶‰åŠ GDL åˆ›å»ºéœ€æ±‚æ—¶æé†’ç”¨æˆ·ç›´æ¥æè¿°éœ€æ±‚å³å¯å¼€å§‹ç”Ÿæˆã€‚"
            "å›å¤ä½¿ç”¨ä¸­æ–‡ï¼Œä¸“ä¸šæœ¯è¯­ä¿ç•™è‹±æ–‡ï¼ˆGDLã€HSFã€ArchiCADã€paramlist ç­‰ï¼‰ã€‚"
        ),
    }
    messages = [system_msg]
    # Include recent history for context (last 6 messages)
    for msg in history[-6:]:
        messages.append({"role": msg["role"], "content": msg["content"]})
    messages.append({"role": "user", "content": user_input})

    try:
        resp = llm.generate(messages)
        return resp.content
    except Exception as e:
        return f"âŒ {str(e)}"


# â”€â”€ Run Agent (shared logic) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_agent_generate(user_input: str, proj: HSFProject, status_col, gsm_name: str = None) -> str:
    """
    Generate code only (no compile). Stores result in session_state.pending_changes.
    Returns a status message for the chat.
    """
    status_ph = status_col.empty()

    def on_event(event_type, data):
        if event_type == "analyze":
            scripts = data.get("affected_scripts", [])
            status_ph.info(f"ğŸ” åˆ†æä¸­... å½±å“è„šæœ¬: {', '.join(scripts)}")
        elif event_type == "attempt":
            status_ph.info("ğŸ§  è°ƒç”¨ AI ç”Ÿæˆä»£ç ...")
        elif event_type == "llm_response":
            status_ph.info(f"âœï¸ æ”¶åˆ° {data['length']} å­—ç¬¦ï¼Œè§£æä¸­...")

    try:
        llm = get_llm()
        knowledge = load_knowledge()
        skills_loader = load_skills()
        skills_text = skills_loader.get_for_task(user_input)

        agent = GDLAgent(llm=llm, compiler=get_compiler(), on_event=on_event)
        changes = agent.generate_only(
            instruction=user_input,
            project=proj,
            knowledge=knowledge,
            skills=skills_text,
        )

        status_ph.empty()

        if not changes:
            return "âŒ AI è¾“å‡ºæ— æ³•è§£æï¼Œè¯·é‡æ–°æè¿°éœ€æ±‚ã€‚"

        # Store for user confirmation
        st.session_state.pending_changes = {
            "gsm_name": gsm_name or proj.name,
            "changes": changes,
            "instruction": user_input,
        }
        return "âœï¸ **AI å·²ç”Ÿæˆä»£ç **ï¼Œè¯·åœ¨å³ä¾§ã€Œå¾…ç¡®è®¤ã€åŒºåŸŸæ£€æŸ¥ï¼Œç¡®è®¤åç‚¹å‡»ç¼–è¯‘ã€‚"

    except Exception as e:
        status_ph.empty()
        return f"âŒ **é”™è¯¯**: {str(e)}"


def compile_pending(proj: HSFProject, gsm_name: str, changes: dict) -> str:
    """Apply pending changes to project and compile. Returns status message."""
    try:
        # Apply changes
        from gdl_agent.core import GDLAgent
        agent = GDLAgent(llm=get_llm(), compiler=get_compiler())
        agent._apply_changes(proj, changes)

        # Versioned output path using user-confirmed name
        output_gsm = _versioned_gsm_path(gsm_name, st.session_state.work_dir)

        # Save & compile
        hsf_dir = proj.save_to_disk()
        result = get_compiler().hsf2libpart(str(hsf_dir), output_gsm)

        mock_tag = " [Mock]" if compiler_mode.startswith("Mock") else ""
        if result.success:
            st.session_state.compile_log.append({
                "project": proj.name,
                "instruction": st.session_state.pending_changes.get("instruction", ""),
                "success": True,
                "attempts": 1,
                "message": "Success",
            })
            msg = f"âœ… **ç¼–è¯‘æˆåŠŸ{mock_tag}**\n\nğŸ“¦ `{output_gsm}`"
            if compiler_mode.startswith("Mock"):
                msg += "\n\nâš ï¸ Mock æ¨¡å¼ä¸ç”ŸæˆçœŸå® .gsmï¼Œåˆ‡æ¢ LP_XMLConverter è¿›è¡ŒçœŸå®ç¼–è¯‘ã€‚"
            return msg
        else:
            st.session_state.compile_log.append({
                "project": proj.name,
                "instruction": "",
                "success": False,
                "attempts": 1,
                "message": result.stderr,
            })
            return f"âŒ **ç¼–è¯‘å¤±è´¥**\n\n```\n{result.stderr[:500]}\n```"

    except Exception as e:
        return f"âŒ **é”™è¯¯**: {str(e)}"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Main Layout: Left Chat | Right Editor
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

col_chat, col_editor = st.columns([2, 3], gap="large")


# â”€â”€ Left: Chat History â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with col_chat:
    if not st.session_state.project:
        st.markdown("### ğŸ’¬ å¼€å§‹åˆ›å»º")
        st.markdown(
            '<p style="color:#64748b; font-size:0.9rem;">åœ¨åº•éƒ¨è¾“å…¥æ¡†æè¿°ä½ æƒ³åˆ›å»ºçš„å¯¹è±¡ï¼ŒAI ä¼šè‡ªåŠ¨ç”Ÿæˆå¹¶ç¼–è¯‘ã€‚</p>',
            unsafe_allow_html=True,
        )
    else:
        proj_now = st.session_state.project
        st.markdown(f"### ğŸ’¬ {proj_now.name}")
        st.caption(f"å‚æ•°: {len(proj_now.parameters)} | è„šæœ¬: {len(proj_now.scripts)}")

    # Chat history
    for msg in st.session_state.chat_history:
        st.chat_message(msg["role"]).markdown(msg["content"])

    # Placeholder for live agent output (populated when agent runs)
    live_output = st.empty()


# â”€â”€ Right: Welcome / Project Editor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

with col_editor:
    # â”€â”€ Pending Confirmation Panel (highest priority) â”€â”€â”€â”€â”€
    if st.session_state.pending_changes and st.session_state.project:
        pc = st.session_state.pending_changes
        proj_now = st.session_state.project

        st.markdown("### â³ AI å·²ç”Ÿæˆä»£ç  â€” è¯·ç¡®è®¤åç¼–è¯‘")

        # Editable GSM name
        confirmed_gsm_name = st.text_input(
            "ğŸ“¦ GSM æ–‡ä»¶åï¼ˆå¯ä¿®æ”¹ï¼‰",
            value=pc["gsm_name"],
            help="ä¿®æ”¹ä¸ºä½ æƒ³è¦çš„æ„ä»¶åç§°ï¼Œç¡®è®¤åç”¨æ­¤åç§°ç¼–è¯‘è¾“å‡º",
            key="pending_gsm_name",
        )

        # Show each changed file with editable text area
        edited_changes = {}
        for file_path, content in pc["changes"].items():
            label = file_path.replace("scripts/", "").replace("paramlist.xml", "å‚æ•° paramlist.xml")
            edited_content = st.text_area(
                f"ğŸ“„ {label}",
                value=content,
                height=200,
                key=f"pending_{file_path}",
            )
            edited_changes[file_path] = edited_content

        col_ok, col_cancel = st.columns([1, 1])
        with col_ok:
            if st.button("âœ… ç¡®è®¤ç¼–è¯‘", type="primary", use_container_width=True):
                # Use user-confirmed name & potentially edited code
                pc["gsm_name"] = confirmed_gsm_name
                pc["changes"] = edited_changes
                with st.spinner("ç¼–è¯‘ä¸­..."):
                    result_msg = compile_pending(proj_now, confirmed_gsm_name, edited_changes)
                st.session_state.chat_history.append({"role": "assistant", "content": result_msg})
                st.session_state.pending_changes = None
                st.rerun()
        with col_cancel:
            if st.button("âŒ æ”¾å¼ƒ", use_container_width=True):
                st.session_state.pending_changes = None
                st.rerun()

        st.divider()

    if not st.session_state.project:
        show_welcome()
    else:
        proj_now = st.session_state.project

        tab_edit, tab_compile, tab_log = st.tabs(["ğŸ“ ç¼–è¾‘", "ğŸ”§ ç¼–è¯‘", "ğŸ“‹ æ—¥å¿—"])

        # â”€â”€ Edit Tab â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab_edit:
            st.markdown("#### å‚æ•°åˆ—è¡¨")
            param_data = [
                {
                    "Type": p.type_tag,
                    "Name": p.name,
                    "Value": p.value,
                    "Description": p.description,
                    "Fixed": "âœ“" if p.is_fixed else "",
                }
                for p in proj_now.parameters
            ]
            if param_data:
                st.dataframe(param_data, use_container_width=True, hide_index=True)
            else:
                st.caption("æš‚æ— å‚æ•°ï¼Œé€šè¿‡å¯¹è¯è®© AI æ·»åŠ ï¼Œæˆ–æ‰‹åŠ¨æ·»åŠ ã€‚")

            with st.expander("â• æ‰‹åŠ¨æ·»åŠ å‚æ•°"):
                pc1, pc2, pc3, pc4 = st.columns(4)
                with pc1:
                    p_type = st.selectbox("Type", [
                        "Length", "Integer", "Boolean", "RealNum", "Angle",
                        "String", "Material", "FillPattern", "LineType", "PenColor",
                    ])
                with pc2:
                    p_name = st.text_input("Name", value="bNewParam")
                with pc3:
                    p_value = st.text_input("Value", value="0")
                with pc4:
                    p_desc = st.text_input("Description")
                if st.button("æ·»åŠ å‚æ•°"):
                    try:
                        proj_now.add_parameter(GDLParameter(p_name, p_type, p_desc, p_value))
                        st.success(f"âœ… {p_type} {p_name}")
                        st.rerun()
                    except Exception as e:
                        st.error(str(e))

            st.divider()
            st.markdown("#### è„šæœ¬")
            script_tabs = st.tabs(["3D", "2D", "Master", "Param", "UI", "Properties"])
            script_map = [
                (ScriptType.SCRIPT_3D, "3d.gdl"),
                (ScriptType.SCRIPT_2D, "2d.gdl"),
                (ScriptType.MASTER, "1d.gdl"),
                (ScriptType.PARAM, "vl.gdl"),
                (ScriptType.UI, "ui.gdl"),
                (ScriptType.PROPERTIES, "pr.gdl"),
            ]
            for tab, (stype, fname) in zip(script_tabs, script_map):
                with tab:
                    current = proj_now.get_script(stype)
                    new_content = st.text_area(
                        fname, value=current, height=280, key=f"script_{fname}",
                    )
                    if new_content != current:
                        proj_now.set_script(stype, new_content)

            if st.button("ğŸ” éªŒè¯å‚æ•°"):
                issues = validate_paramlist(proj_now.parameters)
                if issues:
                    for i in issues:
                        st.warning(i)
                else:
                    st.success("âœ… å‚æ•°éªŒè¯é€šè¿‡")

        # â”€â”€ Compile Tab â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab_compile:
            # Auto versioned output path â€” no manual input needed
            next_gsm = _versioned_gsm_path(proj_now.name, st.session_state.work_dir)
            next_ver = Path(next_gsm).stem  # e.g. MyShelf_v2
            st.caption(f"ğŸ“¦ ä¸‹æ¬¡ç¼–è¯‘è¾“å‡º: `{Path(next_gsm).name}`")

            col_c, col_p = st.columns([1, 1])

            with col_c:
                if st.button("ğŸ”§ æ‰‹åŠ¨ç¼–è¯‘", type="primary"):
                    output_path = _versioned_gsm_path(proj_now.name, st.session_state.work_dir)

                    with st.spinner("å†™å…¥ HSF..."):
                        try:
                            hsf_dir = proj_now.save_to_disk()
                        except Exception as e:
                            st.error(f"å†™å…¥å¤±è´¥: {e}")
                            st.stop()

                    with st.spinner("ç¼–è¯‘ä¸­..."):
                        compiler = get_compiler()
                        result = compiler.hsf2libpart(str(hsf_dir), output_path)

                    if result.success:
                        if compiler_mode.startswith("Mock"):
                            st.success(
                                f"âœ… **[Mock]** ç»“æ„éªŒè¯é€šè¿‡ï¼\n\n"
                                f"ğŸ“ HSF ç›®å½•: `{hsf_dir}`"
                            )
                        else:
                            st.success(f"âœ… ç¼–è¯‘æˆåŠŸï¼\n\nğŸ“¦ `{output_path}`")
                    else:
                        st.error(f"âŒ ç¼–è¯‘å¤±è´¥\n\n```\n{result.stderr}\n```")

                    st.session_state.compile_log.append({
                        "project": proj_now.name,
                        "instruction": "(manual compile)",
                        "success": result.success,
                        "attempts": 1,
                        "message": result.stderr or "Success",
                    })

            with col_p:
                st.markdown("##### é¢„è§ˆ")
                with st.expander("paramlist.xml"):
                    st.code(build_paramlist_xml(proj_now.parameters), language="xml")
                with st.expander("HSF ç›®å½•ç»“æ„", expanded=True):
                    tree = [f"ğŸ“ {proj_now.name}/", "  â”œâ”€â”€ libpartdata.xml",
                            "  â”œâ”€â”€ paramlist.xml", "  â”œâ”€â”€ ancestry.xml", "  â””â”€â”€ scripts/"]
                    for stype in ScriptType:
                        if stype in proj_now.scripts:
                            n = proj_now.scripts[stype].count("\n") + 1
                            tree.append(f"       â”œâ”€â”€ {stype.value} ({n} lines)")
                    st.code("\n".join(tree), language="text")

        # â”€â”€ Log Tab â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with tab_log:
            if not st.session_state.compile_log:
                st.info("æš‚æ— è®°å½•")
            else:
                for entry in reversed(st.session_state.compile_log):
                    icon = "âœ…" if entry["success"] else "âŒ"
                    instr = entry.get("instruction", "")
                    st.markdown(f"**{icon} {entry['project']}** â€” {instr}")
                    if entry.get("attempts", 0) > 1:
                        st.caption(f"å°è¯• {entry['attempts']} æ¬¡")
                    st.code(entry["message"], language="text")
                    st.divider()

            if st.button("æ¸…é™¤æ—¥å¿—"):
                st.session_state.compile_log = []
                st.rerun()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Chat Input â€” Always at Bottom
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

user_input = st.chat_input(
    "æè¿°ä½ æƒ³åˆ›å»ºæˆ–ä¿®æ”¹çš„ GDL å¯¹è±¡ï¼Œå¦‚ã€Œåˆ›å»ºä¸€ä¸ªå®½ 600mm çš„ä¹¦æ¶ï¼ŒiShelves æ§åˆ¶å±‚æ•°ã€"
)

if user_input:
    # Add user message to history
    st.session_state.chat_history.append({"role": "user", "content": user_input})

    # Check API key first
    if not api_key and "ollama" not in model_name:
        err = "âŒ è¯·åœ¨å·¦ä¾§è¾¹æ å¡«å…¥ API Key åå†è¯•ã€‚"
        st.session_state.chat_history.append({"role": "assistant", "content": err})
        st.rerun()
    else:
        llm_for_classify = get_llm()

        # â”€â”€ Intent + Name in ONE call â”€â”€
        intent, gdl_obj_name = classify_and_extract(user_input, llm_for_classify)

        with live_output.container():
            st.chat_message("user").markdown(user_input)
            with st.chat_message("assistant"):
                if intent == "CHAT":
                    # â”€â”€ Casual conversation â€” no project creation, no agent â”€â”€
                    msg = chat_respond(
                        user_input,
                        st.session_state.chat_history[:-1],
                        llm_for_classify,
                    )
                    st.markdown(msg)

                else:
                    # â”€â”€ GDL intent â€” create project if needed, then run agent â”€â”€
                    if not st.session_state.project:
                        new_proj = HSFProject.create_new(gdl_obj_name, work_dir=st.session_state.work_dir)
                        st.session_state.project = new_proj
                        st.info(f"ğŸ“ å·²åˆå§‹åŒ–é¡¹ç›® `{gdl_obj_name}`")

                    proj_current = st.session_state.project
                    msg = run_agent_generate(user_input, proj_current, st.container(), gsm_name=gdl_obj_name)
                    st.markdown(msg)

        st.session_state.chat_history.append({"role": "assistant", "content": msg})
        st.rerun()


# â”€â”€ Footer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.divider()
st.markdown(
    '<p style="text-align:center; color:#64748b; font-size:0.8rem;">'
    'gdl-agent v0.4.1 Â· HSF-native Â· '
    '<a href="https://github.com/byewind/gdl-agent">GitHub</a>'
    '</p>',
    unsafe_allow_html=True,
)
